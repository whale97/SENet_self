{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pdb\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEBlock ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        # Squeeze operation\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        # Excitation operation\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels//reduction_ratio, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels//reduction_ratio, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "        # print(x.size())\n",
    "        # Squeeze and excitation\n",
    "        y=self.squeeze(x).view(batch_size, channels)#squeeze하면 1,1,channel인데 이를 (batch_size, channels) 차원으로 변경\n",
    "        y=self.excitation(y).view(batch_size, channels, 1, 1)\n",
    "        return x*y # feature map 'x'에 대한 Attention 가중치가 적용된 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEBlock_selfattention ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEBlock_self(nn.Module): ## GPT가 알려준 MSA 추가한 Block\n",
    "    def __init__(self, in_channels, reduction_ratio=16, num_heads=4, dropout=0.1):\n",
    "        super(SEBlock_self, self).__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels//reduction_ratio, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels//reduction_ratio, in_channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.multihead_attn = nn.MultiheadAttention(in_channels, num_heads, dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.size())torch.Size([128, 256, 8, 8])\n",
    "\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        y = self.squeeze(x).view(batch_size, channels)\n",
    "        y = self.excitation(y).view(batch_size, channels, 1, 1)\n",
    "        # apply self-attention\n",
    "        x = x.flatten(2).transpose(1, 2)  # (batch_size, seq_len, hidden_dim)\n",
    "        x = self.multihead_attn(x, x, x)[0]\n",
    "        # print(x.size()) torch.Size([128, 64, 256])\n",
    "        x = x.transpose(1, 2).reshape(batch_size, channels, height, width)  # (batch_size, hidden_dim, height, width)\n",
    "        return x * y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BasicBlock 구현 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    mul = 1\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.SEBlock = SEBlock(in_channels=out_planes*self.mul)\n",
    "        self.SEBlock_self = SEBlock_self(in_channels = out_planes*self.mul)\n",
    "        if stride != 1: # stride가 1이 아니라면, identity mapping이 아닌 경우\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes) # 배치 정규화(batch normalization) : conv layer가 끝날 때마다 수행\n",
    "              )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x))) \n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.SEBlock_self(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BottleNeck 구현 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    mul = 4\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super(BottleNeck, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes) \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes) # 배치 정규화(batch normalization) : conv layer가 끝날 때마다 수행\n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_planes, out_planes*self.mul, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes*self.mul)  # 배치 정규화(batch normalization) : conv layer가 끝날 때마다 수행\n",
    "        \n",
    "        \n",
    "        self.shortcut = nn.Sequential() \n",
    "        \n",
    "        self.SEBlock = SEBlock(in_channels=out_planes * self.mul)\n",
    "        self.SEBlock_self = SEBlock_self(in_channels=out_planes * self.mul)\n",
    "        if stride != 1 or in_planes != out_planes*self.mul:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes*self.mul, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes*self.mul)\n",
    "            )\n",
    "    \n",
    "    # 값을 도출하는 함수\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x))) #conv1을 통과후 BN1시행하고 ReLu를 통과시킴\n",
    "        \n",
    "        out = F.relu(self.bn2(self.conv2(out))) #conv2을 통과후 BN2시행하고 ReLu를 통과시킴\n",
    "        out = self.bn3(self.conv3(out)) #conv3을 통과후 BN3시행\n",
    "        # print(out.size()) -> [128, 256, 8, 8]\n",
    "        # print(SEBlock(out))\n",
    "        out = self.SEBlock_self(out)\n",
    "        \n",
    "        out += self.shortcut(x) # (핵심) skip connection\n",
    "        out = F.relu(out) #skip connection을 수행한 결과에 Relu 적용\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64 \n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_planes) \n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) \n",
    "\n",
    "       \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        self.linear = nn.Linear(512 * block.mul, num_classes)\n",
    "\n",
    "\n",
    "    def _make_layer(self, block, out_planes, num_blocks, stride):\n",
    "        \n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "          \n",
    "\n",
    "        layers = [] \n",
    "        for i in range(num_blocks): \n",
    "            layers.append(block(self.in_planes, out_planes, strides[i]))\n",
    "            self.in_planes = block.mul * out_planes \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #conv1\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        #conv2_x\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.layer1(out)\n",
    "\n",
    "        #conv3_x\n",
    "        out = self.layer2(out)\n",
    "\n",
    "        #conv4_x\n",
    "        out = self.layer3(out)\n",
    "\n",
    "        #conv5_x\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        #output을 1*1로 만드는 과정\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out,1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SENet50():\n",
    "    return ResNet(BottleNeck, [3,4,6,3])\n",
    "\n",
    "def SENet10():\n",
    "    return ResNet(BasicBlock, [1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([ \n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(), \n",
    "]) \n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' \n",
    "model = SENet10() \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    autograd.set_detect_anomaly(True)\n",
    "    print('\\n[ Train epoch: %d ]' % epoch)\n",
    "    model.train()\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        #print(torch.any(torch.isnan(inputs)))\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        eps = 1e-6\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        total += targets.size(0)\n",
    "\n",
    "        current_correct = predicted.eq(targets).sum().item()\n",
    "        current_correct_top1 = predicted.eq(targets).sum().item()\n",
    "        current_correct_top5 = torch.topk(outputs, k=5, dim=1)[1].eq(targets.unsqueeze(dim=1)).sum().item()\n",
    "        correct += current_correct\n",
    "        correct_top1 += current_correct_top1\n",
    "        correct_top5 += current_correct_top5\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('\\nCurrent batch:', str(batch_idx))\n",
    "\n",
    "            print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
    "            print('Current batch average train top-1 error:', (1 - current_correct_top1 / targets.size(0)))\n",
    "            print('Current batch average train top-5 error:', (1 - current_correct_top5 / targets.size(0)))\n",
    "            # print(\"Current batch loss.item() : \", loss.item())\n",
    "            # print(\"Current batch targets.size(0) : \", targets.size())\n",
    "            print('Current batch average train loss:', loss.item() / targets.size(0))\n",
    "\n",
    "    print('\\nTotal average train accuracy:', correct / total)\n",
    "    print('Total average train top-1 error:', (1 - correct_top1 / total))\n",
    "    print('Total average train top-5 error:', (1 - correct_top5 / total))\n",
    "    print('Total average train loss:', train_loss / total)\n",
    "\n",
    "    return [(1 - correct_top1 / total), (1 - correct_top5 / total), correct / total, train_loss / total]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0 \n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    top1 = 0\n",
    "    top5 = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        total += targets.size(0)\n",
    "        outputs = model(inputs)\n",
    "        loss += loss_fn(outputs, targets).item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        current_correct = predicted.eq(targets).sum().item()\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        _, pred = outputs.topk(5, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct_tensor = pred.eq(targets.view(1, -1).expand_as(pred))\n",
    "        top1 += correct_tensor[:1].reshape(-1).float().sum(0, keepdim=True)\n",
    "        top5 += correct_tensor[:5].reshape(-1).float().sum(0, keepdim=True)\n",
    "    print('\\nTotal average test accuarcy:', correct / total)\n",
    "    print('Total average test loss:', loss / total)\n",
    "    print('Top-1 error rate:', (1 - top1 / total).item())\n",
    "    print('Top-5 error rate:', (1 - top5 / total).item())\n",
    "    state = {\n",
    "        'model': model.state_dict()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'): \n",
    "        os.mkdir('checkpoint') \n",
    "    file_name = 'SENet50_cifar10.pth'\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved!')\n",
    "    return [(1 - top1 / total).item(),  (1 - top5 / total).item(), correct / total, loss / total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 50: \n",
    "        lr /= 10\n",
    "    if epoch >= 100: \n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ Train epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current batch average train accuracy: 0.0625\n",
      "Current batch average train top-1 error: 0.9375\n",
      "Current batch average train top-5 error: 0.4765625\n",
      "Current batch average train loss: 0.018503447994589806\n",
      "\n",
      "Current batch: 100\n",
      "Current batch average train accuracy: 0.296875\n",
      "Current batch average train top-1 error: 0.703125\n",
      "Current batch average train top-5 error: 0.1640625\n",
      "Current batch average train loss: 0.013955810107290745\n",
      "\n",
      "Current batch: 200\n",
      "Current batch average train accuracy: 0.3984375\n",
      "Current batch average train top-1 error: 0.6015625\n",
      "Current batch average train top-5 error: 0.1171875\n",
      "Current batch average train loss: 0.013011811301112175\n",
      "\n",
      "Current batch: 300\n",
      "Current batch average train accuracy: 0.359375\n",
      "Current batch average train top-1 error: 0.640625\n",
      "Current batch average train top-5 error: 0.078125\n",
      "Current batch average train loss: 0.012232858687639236\n",
      "\n",
      "Total average train accuracy: 0.34674\n",
      "Total average train top-1 error: 0.65326\n",
      "Total average train top-5 error: 0.13902000000000003\n",
      "Total average train loss: 0.013666730437278748\n",
      "[0.013666730437278748] [0.34674]\n",
      "\n",
      "[ Test epoch: 0 ]\n",
      "\n",
      "Total average test accuarcy: 0.3979\n",
      "Total average test loss: 0.016204498207569123\n",
      "Top-1 error rate: 0.6021000146865845\n",
      "Top-5 error rate: 0.10040003061294556\n",
      "Model Saved!\n",
      "[0.016204498207569123] [0.3979]\n",
      "\n",
      "Time elapsed: 27.120073556900024\n",
      "\n",
      "[ Train epoch: 1 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current batch average train accuracy: 0.375\n",
      "Current batch average train top-1 error: 0.625\n",
      "Current batch average train top-5 error: 0.1171875\n",
      "Current batch average train loss: 0.01260833628475666\n",
      "\n",
      "Current batch: 100\n",
      "Current batch average train accuracy: 0.375\n",
      "Current batch average train top-1 error: 0.625\n",
      "Current batch average train top-5 error: 0.09375\n",
      "Current batch average train loss: 0.011673039756715298\n",
      "\n",
      "Current batch: 200\n",
      "Current batch average train accuracy: 0.5078125\n",
      "Current batch average train top-1 error: 0.4921875\n",
      "Current batch average train top-5 error: 0.09375\n",
      "Current batch average train loss: 0.011292912997305393\n",
      "\n",
      "Current batch: 300\n",
      "Current batch average train accuracy: 0.4765625\n",
      "Current batch average train top-1 error: 0.5234375\n",
      "Current batch average train top-5 error: 0.0703125\n",
      "Current batch average train loss: 0.011045246385037899\n",
      "\n",
      "Total average train accuracy: 0.47496\n",
      "Total average train top-1 error: 0.52504\n",
      "Total average train top-5 error: 0.07065999999999995\n",
      "Total average train loss: 0.011101458115577698\n",
      "[0.013666730437278748, 0.011101458115577698] [0.34674, 0.47496]\n",
      "\n",
      "[ Test epoch: 1 ]\n",
      "\n",
      "Total average test accuarcy: 0.4805\n",
      "Total average test loss: 0.014403724360466004\n",
      "Top-1 error rate: 0.5195000171661377\n",
      "Top-5 error rate: 0.07039999961853027\n",
      "Model Saved!\n",
      "[0.016204498207569123, 0.014403724360466004] [0.3979, 0.4805]\n",
      "\n",
      "Time elapsed: 54.40348148345947\n",
      "\n",
      "[ Train epoch: 2 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current batch average train accuracy: 0.4375\n",
      "Current batch average train top-1 error: 0.5625\n",
      "Current batch average train top-5 error: 0.0546875\n",
      "Current batch average train loss: 0.01136763021349907\n",
      "\n",
      "Current batch: 100\n",
      "Current batch average train accuracy: 0.5078125\n",
      "Current batch average train top-1 error: 0.4921875\n",
      "Current batch average train top-5 error: 0.0390625\n",
      "Current batch average train loss: 0.010221537202596664\n",
      "\n",
      "Current batch: 200\n",
      "Current batch average train accuracy: 0.515625\n",
      "Current batch average train top-1 error: 0.484375\n",
      "Current batch average train top-5 error: 0.078125\n",
      "Current batch average train loss: 0.010337939485907555\n",
      "\n",
      "Current batch: 300\n",
      "Current batch average train accuracy: 0.546875\n",
      "Current batch average train top-1 error: 0.453125\n",
      "Current batch average train top-5 error: 0.0234375\n",
      "Current batch average train loss: 0.009119988419115543\n",
      "\n",
      "Total average train accuracy: 0.53538\n",
      "Total average train top-1 error: 0.46462000000000003\n",
      "Total average train top-5 error: 0.05445999999999995\n",
      "Total average train loss: 0.009985235900878906\n",
      "[0.013666730437278748, 0.011101458115577698, 0.009985235900878906] [0.34674, 0.47496, 0.53538]\n",
      "\n",
      "[ Test epoch: 2 ]\n",
      "\n",
      "Total average test accuarcy: 0.3461\n",
      "Total average test loss: 0.021932156908512114\n",
      "Top-1 error rate: 0.6539000272750854\n",
      "Top-5 error rate: 0.22200000286102295\n",
      "Model Saved!\n",
      "[0.016204498207569123, 0.014403724360466004, 0.021932156908512114] [0.3979, 0.4805, 0.3461]\n",
      "\n",
      "Time elapsed: 81.90258836746216\n",
      "\n",
      "[ Train epoch: 3 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current batch average train accuracy: 0.59375\n",
      "Current batch average train top-1 error: 0.40625\n",
      "Current batch average train top-5 error: 0.0390625\n",
      "Current batch average train loss: 0.008728496730327606\n",
      "\n",
      "Current batch: 100\n",
      "Current batch average train accuracy: 0.5\n",
      "Current batch average train top-1 error: 0.5\n",
      "Current batch average train top-5 error: 0.03125\n",
      "Current batch average train loss: 0.009923332370817661\n",
      "\n",
      "Current batch: 200\n",
      "Current batch average train accuracy: 0.5625\n",
      "Current batch average train top-1 error: 0.4375\n",
      "Current batch average train top-5 error: 0.0546875\n",
      "Current batch average train loss: 0.009389862418174744\n",
      "\n",
      "Current batch: 300\n",
      "Current batch average train accuracy: 0.59375\n",
      "Current batch average train top-1 error: 0.40625\n",
      "Current batch average train top-5 error: 0.03125\n",
      "Current batch average train loss: 0.008831528946757317\n",
      "\n",
      "Total average train accuracy: 0.57114\n",
      "Total average train top-1 error: 0.42886\n",
      "Total average train top-5 error: 0.04625999999999997\n",
      "Total average train loss: 0.009245743256807328\n",
      "[0.013666730437278748, 0.011101458115577698, 0.009985235900878906, 0.009245743256807328] [0.34674, 0.47496, 0.53538, 0.57114]\n",
      "\n",
      "[ Test epoch: 3 ]\n",
      "\n",
      "Total average test accuarcy: 0.5729\n",
      "Total average test loss: 0.012308671456575394\n",
      "Top-1 error rate: 0.4271000027656555\n",
      "Top-5 error rate: 0.06200003623962402\n",
      "Model Saved!\n",
      "[0.016204498207569123, 0.014403724360466004, 0.021932156908512114, 0.012308671456575394] [0.3979, 0.4805, 0.3461, 0.5729]\n",
      "\n",
      "Time elapsed: 109.32922124862671\n",
      "\n",
      "[ Train epoch: 4 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current batch average train accuracy: 0.546875\n",
      "Current batch average train top-1 error: 0.453125\n",
      "Current batch average train top-5 error: 0.0390625\n",
      "Current batch average train loss: 0.009094095788896084\n",
      "\n",
      "Current batch: 100\n",
      "Current batch average train accuracy: 0.671875\n",
      "Current batch average train top-1 error: 0.328125\n",
      "Current batch average train top-5 error: 0.03125\n",
      "Current batch average train loss: 0.0075537520460784435\n",
      "\n",
      "Current batch: 200\n",
      "Current batch average train accuracy: 0.515625\n",
      "Current batch average train top-1 error: 0.484375\n",
      "Current batch average train top-5 error: 0.0546875\n",
      "Current batch average train loss: 0.010017105378210545\n",
      "\n",
      "Current batch: 300\n",
      "Current batch average train accuracy: 0.5703125\n",
      "Current batch average train top-1 error: 0.4296875\n",
      "Current batch average train top-5 error: 0.0546875\n",
      "Current batch average train loss: 0.009325140155851841\n",
      "\n",
      "Total average train accuracy: 0.60268\n",
      "Total average train top-1 error: 0.39732\n",
      "Total average train top-5 error: 0.04042000000000001\n",
      "Total average train loss: 0.00868097532749176\n",
      "[0.013666730437278748, 0.011101458115577698, 0.009985235900878906, 0.009245743256807328, 0.00868097532749176] [0.34674, 0.47496, 0.53538, 0.57114, 0.60268]\n",
      "\n",
      "[ Test epoch: 4 ]\n",
      "\n",
      "Total average test accuarcy: 0.5579\n",
      "Total average test loss: 0.013118842667341233\n",
      "Top-1 error rate: 0.44209998846054077\n",
      "Top-5 error rate: 0.0625\n",
      "Model Saved!\n",
      "[0.016204498207569123, 0.014403724360466004, 0.021932156908512114, 0.012308671456575394, 0.013118842667341233] [0.3979, 0.4805, 0.3461, 0.5729, 0.5579]\n",
      "\n",
      "Time elapsed: 136.67600870132446\n",
      "\n",
      "[ Train epoch: 5 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current batch average train accuracy: 0.6171875\n",
      "Current batch average train top-1 error: 0.3828125\n",
      "Current batch average train top-5 error: 0.046875\n",
      "Current batch average train loss: 0.00845540314912796\n",
      "\n",
      "Current batch: 100\n",
      "Current batch average train accuracy: 0.5390625\n",
      "Current batch average train top-1 error: 0.4609375\n",
      "Current batch average train top-5 error: 0.015625\n",
      "Current batch average train loss: 0.009130292572081089\n",
      "\n",
      "Current batch: 200\n",
      "Current batch average train accuracy: 0.59375\n",
      "Current batch average train top-1 error: 0.40625\n",
      "Current batch average train top-5 error: 0.0234375\n",
      "Current batch average train loss: 0.008051770739257336\n",
      "\n",
      "Current batch: 300\n",
      "Current batch average train accuracy: 0.6015625\n",
      "Current batch average train top-1 error: 0.3984375\n",
      "Current batch average train top-5 error: 0.0625\n",
      "Current batch average train loss: 0.008208082057535648\n"
     ]
    }
   ],
   "source": [
    "import time #시간과 관련된 기능을 제공하는 모듈\n",
    "\n",
    "start_time = time.time() #시작시간을 측정\n",
    "\n",
    "train_losses=[]\n",
    "train_accuracies=[]\n",
    "train_top1_err=[]\n",
    "train_top5_err=[]\n",
    "test_losses=[]\n",
    "test_accuracies=[]\n",
    "test_top1_err=[]\n",
    "test_top5_err=[]\n",
    "#0번째 에폭부터 149번째 에폭까지 반복하면서\n",
    "#모델학습(train)과 테스트수행\n",
    "for epoch in range(0, 150):\n",
    "    #optimizer의 학습률을 조정하고\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    #학습(train)\n",
    "    x=train(epoch)\n",
    "    train_losses.append(x[3])\n",
    "    train_accuracies.append(x[2])\n",
    "    train_top1_err.append(x[0])\n",
    "    train_top5_err.append(x[1])\n",
    "    print(train_losses, train_accuracies)\n",
    "    #테스트(test)\n",
    "    y=test(epoch)\n",
    "    test_losses.append(y[3])\n",
    "    test_accuracies.append(y[2])\n",
    "    test_top1_err.append(y[0])\n",
    "    test_top5_err.append(y[1])\n",
    "    print(test_losses, test_accuracies)\n",
    "    #매 에폭이 끝날때마다 학습에 소요된 시간을 계산해서(현재시간 - 시작시간) 출력\n",
    "    print('\\nTime elapsed:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_top1_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_top5_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_top1_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_top5_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
